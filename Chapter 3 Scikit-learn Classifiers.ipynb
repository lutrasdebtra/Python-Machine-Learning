{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "The most basic classifer is the simple perceptron, which is better optimized in sklearn. \n",
    "\n",
    "The resulting accuracy (0.91) is due to the fact that the Iris dataset is not linearly seperable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target # Stored as integers for better performance.\n",
    "\n",
    "# 30% test data, 70% train.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Standardise features for gradient decent.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Perceptron.\n",
    "ppn = Perceptron(max_iter = 40, eta0=0.1, random_state = 0)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict.\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Given a set of features $\\mathbf{x}$, and weights $\\mathbf{w}$ then the conditional probability $y$ belongs to class $1$ is:\n",
    "\n",
    "$$\n",
    "logit(p(y=1|\\mathbf{x}))=\\sum_{i=0}^{m}w_{i}x_{i}\n",
    "$$\n",
    "\n",
    "From here, the inverse function gives the probability of a sample belonging to a particular class:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-\\sum_{i=0}^{m}w_{i}x_{i}}}\n",
    "$$\n",
    "\n",
    "The resulting function approaches 1 as $z \\rightarrow \\infty$ and 0 as $z \\rightarrow - \\infty$. The result is classification in the following form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\Bigg \\{ \\begin{array}{c} 1 \\; \\textrm{if} \\; z \\geq 0.5 \\\\ 0 \\; otherwise \\end{array}\n",
    "$$\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "To avoid building a model that does not generalise well, the complexity must not be too high to overfit, nor too low to overfit. Regularization is used to penalise extreme parameter weights that might be causing complexity issues. The regularization parameter $\\lambda$ controls how this behaviour works - but sklearn takes its inverse: \n",
    "\n",
    "$$\n",
    "C = \\frac{1}{\\lambda}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target # Stored as integers for better performance.\n",
    "\n",
    "# 30% test data, 70% train.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Standardise features for gradient decent.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# LogisticRegression. C denotes regularization strength (smaller numbers are stronger).\n",
    "lr = LogisticRegression(C=1000.0, random_state = 0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict.\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Where a perceptrons tries to a define a single hyperplane that seperates two classes, SVMs also attempt to maximise a margin around the hyperplane to lower the generalization error. \n",
    "\n",
    "Where data is nonlinearly separable, the slack variable $\\xi$ which allows for the control of penalisation for misclassification. Like with regularization, increasing $C$ increases bias and lowers variance in a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target # Stored as integers for better performance.\n",
    "\n",
    "# 30% test data, 70% train.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Standardise features for gradient decent.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Support Vector Machine. C denotes slack strength (smaller numbers introduce more slack).\n",
    "svm = SVC(kernel='linear', C=1.0, random_state = 0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "\"\"\"For online learning via partial fit use:\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svm = SGDClassifier(loss='hinge') # loss='log' creates a logistic regression. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"For hyperplanes in higher dimensions use:\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma = 1.0 random_state = 0)\n",
    "\n",
    "High gamma results in overfitting, and low in underfitting the Gaussian sphere. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Predict.\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning \n",
    "\n",
    "This form of classifier sorts samples into classes by asking a series of questions. Given a set of features, the model learns the optimal questions to ask to quickly divide the data.\n",
    "\n",
    "This is done by optimising Information Gain for each decision, followed by pruning to reduce overfitting. There are three main splitting criteria:\n",
    "1. Entropy - the proportion of samples belonging to clas $i$ in node $t$, which is minimised by having all samples in the node belong to $i$, and maximised by a uniform distribution of classes.\n",
    "2. Gini Impurity - Minimisation of the probability of misclassification. \n",
    "3. Classification Error - Overall number of error of misclassification for nodes. Not as sensitive to changes in class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target # Stored as integers for better performance.\n",
    "\n",
    "# 30% test data, 70% train.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Standardise features - not required for DTs, but good for visualisation.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Decision Tree - max_depth helps reduce overfitting.\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3, random_state = 0)\n",
    "tree.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict.\n",
    "y_pred = tree.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "A random forest is an ensemble of decision trees, which combines many weak learners into one strong one. The result is better generalization error and the model being less prone to overfitting. The algorithm works as follows:\n",
    "1. Draw $n$ random samples.\n",
    "2. Grow a decision tree, at each node:\n",
    "   1. Randomly select $d$ features.\n",
    "   2. Split the node using the feature that gives the best IG.\n",
    "3. Repeat 1 to 2 $k$ times.\n",
    "4. Aggregate trees via majority vote. \n",
    "\n",
    "The reason for using a random forest is it doesn't rely as heavily on good hyper parameters, as noise from individual trees is well controlled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target # Stored as integers for better performance.\n",
    "\n",
    "# 30% test data, 70% train.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Standardise features - not required for DTs, but good for visualisation.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Random Forest (low n_estimators results in poorer performance than previous models).\n",
    "# n_jobs is number of CPU cores to use. \n",
    "forest = RandomForestClassifier(criterion = 'entropy', n_estimators = 100, random_state = 1, n_jobs = 4)\n",
    "forest.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict.\n",
    "y_pred = forest.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest Neighbour Clustering\n",
    "\n",
    "KNN is a lazy learning algorithm, what this means is that memorizes the data instead of building a function from it. The algorithm works in the following way:\n",
    "1. Choose $k$ and a distance metric.\n",
    "2. Find the $k$ nearest neighbours of a sample.\n",
    "3. Assign the class label via majority vote. \n",
    "This approaches means that the algorithm can immediately adapt to new samples, but requires increasingly more computational power.\n",
    "\n",
    "In this approach, the choice of $k$ and the distance metric is very important. Standard metrics include the Euclidean or Manhattan distances, which are generalised as the minkowski distance with $p=[1,2]$.\n",
    "\n",
    "In very spare high dimensional datasets, feature selection and dimensionality reduction is required to close distances between neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target # Stored as integers for better performance.\n",
    "\n",
    "# 30% test data, 70% train.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# Standardise features - not required for DTs, but good for visualisation.\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# KNeighbours classifier - p =2 is Euclidean distance. \n",
    "knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric = 'minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict.\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
