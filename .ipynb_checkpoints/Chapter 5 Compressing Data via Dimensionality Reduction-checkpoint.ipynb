{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction works by summarizing higher dimensional information in a smaller space. There are three main approaches:\n",
    "1. Principal component analysis (PCA) - Used for unsupervised data compression.\n",
    "2. Linear Discriminant Analysis (LDA) - Used for supervised dimensionality reduction which maximises class separability. \n",
    "3. Kernel principal component analysis - Used for nonlinear dimensionality reduction. \n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA works by finding the directions of maximum variance in high-dimensional data before projecting it onto a smaller subspace. The result is the principal components are ordered by their variances.\n",
    "\n",
    "The algorithm itself works as follows:\n",
    "1. Standardize the $d$-dimensional dataset.\n",
    "2. Construct the covariance matrix.\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Select the top $k$ eigenvectors that correspond to the $k$ largest eigenvalues.\n",
    "5. Construct a projection matrix using $k$ eigenvectors.\n",
    "6. Transform the $d$-dimensional dataset using the projection matrix. \n",
    "\n",
    "In sklearn it looks something like this:\n",
    "```\n",
    "from sklean.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "stds = StandardScaler()\n",
    "X_train_stds = stds.fit_transform(X_train)\n",
    "X_test_stds = stds.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=2) # n_components=None results in all components being returned.\n",
    "X_train_pca = pca.fit_transform(X_train_stds)\n",
    "X_test_pca = pca.transform(X_test_stds)\n",
    "```\n",
    "\n",
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Like PCA, LDA is a linear transformation. However, this case the end goal is to optimize class seperability. While LDA is better in many cases, PCA can often perform better on classes with small numbers of samples. \n",
    "\n",
    "The algorithm itself works as follows:\n",
    "1. Standardize the $d$-dimensional dataset.\n",
    "2. For each class, compute the $d$-dimensional mean vector.\n",
    "3. Construct the between-class scatter matrix, and the within-class scatter matrix.\n",
    "4. Compute the eignvectors and eigenvalues of the inverse within-class multiplied by the between class matrix.\n",
    "5. Select the top $k$ eigenvectors that correspond to the $k$ largest eigenvalues.\n",
    "6. Construct a projection matrix using $k$ eigenvectors.\n",
    "7. Transform the $d$-dimensional dataset using the projection matrix. \n",
    "\n",
    "LDA assumes normal distributions and indepence; as well as identical covariance matrices. However, violation of these assumptions can still result in useful data. Although falling back to PCA might be advised. \n",
    "\n",
    "In sklearn it looks something like this:\n",
    "```\n",
    "from sklean.preprocessing import StandardScaler\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "stds = StandardScaler()\n",
    "X_train_stds = stds.fit_transform(X_train)\n",
    "X_test_stds = stds.transform(X_test)\n",
    "\n",
    "pca = LDA(n_components=2) # n_components=None results in all components being returned.\n",
    "X_train_lda = lda.fit_transform(X_train_stds, y_train)\n",
    "X_test_lda = lda.transform(X_test_stds)\n",
    "```\n",
    "\n",
    "## Kernel Principal Component Analysis\n",
    "\n",
    "Where data seperability is a non-linear problem, the first step is to use kernels to project into a higher dimensionality space where data is linearly seperable before applying PCA. \n",
    "\n",
    "```\n",
    "from sklearn.decomposition import KernelPCA\n",
    "scikit_kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "X_skernpca = scikit_kpca.fit_transform(X_train)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
