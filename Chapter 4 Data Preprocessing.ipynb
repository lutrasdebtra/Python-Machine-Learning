{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "## Drop Rows or Columns\n",
    "\n",
    "### Drop rows with missing data:\n",
    "```\n",
    "df.dropna()\n",
    "```\n",
    "### Drop columns with missing data:\n",
    "```\n",
    "df.dropna(axis=1)\n",
    "```\n",
    "\n",
    "### Other options:\n",
    "```\n",
    "df.dropna(how='all') # Drops rows where ALL data is missing.\n",
    "df.dropnz(thresh=4) # Drops rows with at least 4 missing values.\n",
    "df.dropna(subset=['C']) drops rows with NaN in 'C'.\n",
    "```\n",
    "\n",
    "## Imputing Missing Values\n",
    "\n",
    "Often it's better to replace `NaN` with some value, to avoid removing too much data. This is where the `Imputer` class comes into play:\n",
    "```\n",
    "from sklearn.preprocessing import Imputer\n",
    "imr = Imputer(missing_values='NaN', strategy='mean', axis=0) # axis=1 for row means. strategy=['mean',  \n",
    "                                                              'median','most_frequent']\n",
    "imr.fit(df)\n",
    "imputed_data = imr.transform(df.values)\n",
    "```\n",
    "\n",
    "# Handling Categorical Data\n",
    "\n",
    "Categorical non-numerical features can be broken down into ordinal (e.g. t-shirt size), and nominal (t-short colour).\n",
    "\n",
    "In the ordinal case, the mapping can be done on a single column (as higher numbers will have meaning):\n",
    "```\n",
    "size_mapping = {\n",
    "    'XL': 3,\n",
    "    'L' : 2,\n",
    "    'M' : 1\n",
    "}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "```\n",
    "\n",
    "In the nominal case, dummy features need to be created to avoid the incorrect interpretation of number order:\n",
    "```\n",
    "import pandas as pd\n",
    "pd.get_dummies(df) # Ignores numerical fields, can work on a subset of the dataframe. \n",
    "```\n",
    "\n",
    "# Partitioning into Training and Test Sets\n",
    "```\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "```\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "For the majority of ML algorithms (decision trees being an exception), they handle the data much better if features are all in the same scale. There are two main approaches:\n",
    "1. Normalization - $x^{i}_{norm} = \\frac{x^{i}-x_{min}}{x_{max}-x_{min}}$ - Places values in the range $[0,1]$.\n",
    "2. Standardization - $x^{i}_{std} = \\frac{x^{i}-\\mu_{x}}{\\sigma_{x}}$ - Places values in a normal distribution. This is often more useful, as it maintains outlier information while making them weaker.\n",
    "\n",
    "In both cases, the code is very similar:\n",
    "```\n",
    "from sklean.preprocessing import MinMaxScaler, StandardScaler\n",
    "mms, stds = MinMaxScaler(), StandardScaler()\n",
    "X_train_norm = mms.fit_transform(X_train) # Fit only occurs on training.\n",
    "X_test_std = mms.transform(X_test)\n",
    "```\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "There are a number of methods of feature selection, such as L1 regularization for Logistic Regression (which encourages sparsity). Additionally, the SBS algorithm (not in sklearn) and many others (which are) can be used for direct selection before an algorithm. Random Forests even have a `_feature_importance` parameter to see which features are chosen in the fitting process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
