{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning \n",
    "\n",
    "The idea behind ensemble learning is to combine multiple models to give better generalised performance. The most general way to do this is by majority voting; this works by picking the class label most common amoung all the classifiers:\n",
    "\n",
    "$$\n",
    "\\hat{y} = mode(C_{1}(\\mathbf{x}), C_{2}(\\mathbf{x}),...,C_{m}(\\mathbf{x})) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.95 (+/- 0.05) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.05) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "# voting='hard' uses predicted class labels, 'soft' uses predicted class probabilities.\n",
    "# the weights parameter can be used to give additional strength to classifiers\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging \n",
    "\n",
    "Bagging is a form of majority voting that works by aggregating the results of a single model on multiple subsets of data. This is akin to cross validation, if all training sets informed the final algorithm. \n",
    "\n",
    "Bagging is a good way to reduce overfitting, and works well on already developed models. \n",
    "```\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "```\n",
    "\n",
    "## Adaptive Boosting \n",
    "\n",
    "In Adaptive boosting, a number of simple classifiers (AKA weak learners) are iteratively trained on the data. As the iterations progress, samples are weighted by their difficultly, resulting in the most difficult samples being focused on more heavily. \n",
    "\n",
    "However, repeated usage of the training set can result in poor generalization error, and ADA is not particularly parallelizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95996732026143794"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "# The base_estimator changes the type of weak learner (the default is the DecisionTreeClassifier). \n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target)\n",
    "scores.mean()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Complexity\n",
    "\n",
    "While ensemble methods do often increase the accuracy of a model. They often do so at a large amount on increased complexity, which in many cases doesn't translate into real world applications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
