{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines allow for easily chaining together multiple transformations as well as models. For example:\n",
    "```\n",
    "from sklearn.preprocessing import StandardScalar\n",
    "from sklearn.decomposition import PCA\n",
    "from sklean.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scl', StandardScalar()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('clf', LogisticRegression(random_state=0))\n",
    "])\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "# Assessing Model Performance\n",
    "\n",
    "There are two good techniques for cross-validation, which allow for estimating generalization error. \n",
    "\n",
    "## Holdout\n",
    "\n",
    "Data is seperated into training, validation and test sets. The validation set is used during model selection for training and tuning, while the test set helps emulate generalization error. \n",
    "\n",
    "The problem with this method is that it is sensitive to how the samples are split up, which makes the next method more robust in comparision. \n",
    "\n",
    "## K-fold\n",
    "\n",
    "Data is split into $k$ folds without replacement, where $k-1$ folds are used for training, and one for testing. This procedure is completed $k$ (usually 10) times to gain $k$ models, and $k$ estimates.\n",
    "\n",
    "Because the procedure is repeated, the splits will change, and individual samples have less strength (since they may be in both training and test sets). \n",
    "\n",
    "For smaller datasets, one sample can be left out as the test set - this is called leave-one-out (LOO) cross-validation.\n",
    "```\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklean.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "# Pipelines can be used as fell. \n",
    "scores = cross_val_score(estimator=lr,\n",
    "                            X=X_train,\n",
    "                            y=y_train,\n",
    "                            cv=10,\n",
    "                            n_jobs=1)\n",
    "                            \n",
    "```\n",
    "\n",
    "## Grid Search \n",
    "\n",
    "While parameters learned from the dataset can be optimized by the algorithm, external parameters (say depth of a decision tree) need to be optimized seperately.\n",
    "\n",
    "Grid search works by trying sets of external parameters, finding the optimal combination. \n",
    "```\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = {\n",
    "    'C': param_range,\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': param_range\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(estimator=svc,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=10,\n",
    "                    n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_, gs.best_params)\n",
    "```\n",
    "\n",
    "The one downside to this method is that it can be very computationally intensive. `RandomizedSearchCV` can randomly draw parameters from distributions which can often be a faster approach. \n",
    "\n",
    "Grid search can also be combined with K-fold from above once the optimal parameters are found, to tune the dataset parameters. \n",
    "\n",
    "# Finding the Best Classifier\n",
    "\n",
    "When the decsion to use a specific classifier isn't known, it can be smart to test multiple to find the most ideal one to start parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.766969 (0.035426)\n",
      "LDA: 0.773496 (0.034665)\n",
      "KNN: 0.721377 (0.044168)\n",
      "CART: 0.704375 (0.064984)\n",
      "NB: 0.756494 (0.033037)\n",
      "SVM: 0.651059 (0.003418)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=10, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
